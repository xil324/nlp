# 正则模型（Regex）
纯规则化匹配方案，通过预编译的关键词正则表达式硬匹配文本，命中则分类，未命中归为Other，无训练 / 推理计算过程。
- 核心优势
推理延迟极致低：纯内存正则匹配，耗时为微秒级，远低于项目 400ms 延迟要求，可作为车载场景前置快速处理环节；
部署极致轻量化：仅依赖 Python 原生re库，无任何框架 / 算力 / 显存要求，车机端、嵌入式设备均可轻松集成，适配各类车载硬件；
结果绝对可控无不确定性：匹配规则人工定义，无幻觉、无误判风险，符合汽车电子场景对稳定性、可靠性的高要求；
线上问题快速修复：出现分类错误时，直接修改 / 新增正则规则即可，无需重新训练模型，调试与线上维护成本极低。
- 核心缺点
准确率难以达到 95%，泛化能力为 0：车载语音输入存在口语化、语序颠倒、省略语、口语化错别字等特点，20 + 核心意图需维护数百条规则仍会大量遗漏边缘场景，未命中均归为Other，易导致用户重复指令；
规则维护成本随意图数指数上升：新增 / 修改意图需重新编写规则，且易出现规则冲突（如 “空调” 同时出现在空调调节、闲聊场景），20 + 核心意图的长期维护成本极高；
代码存在原生 bug，影响实际落地效果：批量推理时误将整个文本列表作为单条文本匹配，导致列表输入分类全部错误，需修复后才能实际使用；
无数据支撑能力：仅能输出简单类别标签，无意图预测概率、语义特征等信息，无法为智能客服优化、市场需求分析提供任何数据支持；
无法处理复杂表达：对车载场景中包含多关键词的复杂口语化表达（如 “帮我导航到北京顺便把空调调 26 度”），无法精准匹配单一意图。
# TF-IDF 模型
传统机器学习方案，经 Jieba 分词→去停用词→TF-IDF 词袋向量化→传统分类模型（如 SVM / 逻辑回归）推理，依赖词频特征做分类。
- 核心优势
推理延迟低，满足车载延迟要求：分词、向量化、传统模型推理均为轻量计算，单次推理耗时 **<50ms**，批量推理效率更高，完全符合 < 400ms 的项目要求；
泛化能力优于正则，训练成本低：无需人工定义规则，对车载语音的简单口语化、语序颠倒表达有基础泛化能力；20 + 核心意图仅需数千条标注样本即可训练，训练过程快，无高算力要求；
车载端部署友好：仅依赖 Jieba、Scikit-learn、Joblib 轻量库，模型文件体积小（一般 < 100MB），可直接打包部署在车机端，无框架依赖；
具备基础数据支持能力：可输出各意图的预测概率，还能通过 TF-IDF 特征分析用户输入的高频关键词，为市场分析提供基础的意图分布数据；
工程实现简单：代码逻辑清晰，无复杂的深度学习框架依赖，车载工程团队易集成、易调试。
- 核心缺点
准确率难以突破 95%，语义理解能力缺失：TF-IDF 为词袋模型，丢失上下文语义、语序、词间关联信息，对车载意图中语义相近但关键词重叠的场景（如 “空调调温” vs “空调清洗”）易误判，20 + 意图下准确率一般在 85%~92%；
强依赖分词效果，口语化文本处理差：车载语音的口语化表达（如 “开个空调呗”）、口语错别字（如 “导肮”）易导致 Jieba 分词错误，且去停用词环节可能误删核心特征词，一步错导致后续分类全错；
对未登录词、新表达泛化能力差：若用户输入车载场景新表达（如 “空调吹脸”“导航避堵”），训练集中无相似词则无法提取有效特征，大概率误判为Other；
特征工程繁琐，模型泛化上限低：需人工调优分词、停用词、TF-IDFngram 等参数，即使做极致特征工程，传统词袋模型的语义理解上限远低于深度学习 / 大模型，无法满足 95%+ 的准确率要求；
扩展意图需重新训练：新增车载意图需补充标注样本后重新训练整个模型，无法快速适配业务迭代。
# BERT 模型
深度学习预训练模型方案，基于 HuggingFace 框架实现，经 BERT 预训练模型微调序列分类，通过 Logits 最大索引确定意图，属于语义级分类方案。
- 核心优势
准确率轻松突破 95%，语义理解能力强：能捕捉文本的上下文语义、语序、词间关联，对车载语音的口语化、语序颠倒、省略语、复杂表达有极佳泛化能力；20 + 核心意图下，经数千条车载口语化样本微调后，准确率可达96%~99%，能有效减少用户重复指令；
推理延迟可优化至项目要求内：原生 BERT 可通过轻量化（Base→MobileBERT/BERT-tiny）+ 量化（FP32→INT8/FP16）+ ONNX/TensorRT 工程优化，将单次推理延迟控制在 < 200ms，批量推理平均延迟更低，完全满足 < 400ms 要求；
车载端本地部署可控：优化后的模型可导出为车载芯片兼容格式，支持高通 8155/8295、地平线征程等主流车载 AI 芯片本地部署，无网络依赖，规避车载场景（高速、地下车库）网络波动的服务中断风险；
20 + 核心意图适配性好，扩展成本低：新增意图仅需补充少量标注样本重新微调模型即可，无需重构模型结构，普通显卡数小时即可完成微调，适配业务快速迭代；
数据支持能力强，贴合项目多维度需求：可输出各意图的预测概率、语义特征向量，能分析用户意图分布、高频需求、边缘场景，为智能客服话术优化、市场分析的产品功能迭代提供全方位数据支撑；
工程化实现标准化：基于 HuggingFace 框架的代码可复用性高，支持批量推理、多线程处理，易集成到车载语音助手的工程系统中，后续维护与升级成本低。
- 核心缺点
需要高质量的车载场景标注数据集：模型准确率依赖标注数据的质量与数量，20 + 核心意图一般需要 5000~10000 条车载口语化文本标注样本，无标注数据则无法完成微调；
对车载芯片有轻微算力要求：轻量化 / 量化后的 BERT 需要车载芯片具备基础 AI 推理能力（1TOPS 以上），低端入门级车机芯片若无 AI 算力则无法部署；
推理延迟略高于正则 / TF-IDF：虽能满足项目延迟要求，但微秒级 / 50ms 级的正则 / TF-IDF 相比，BERT 的推理延迟仍更高，无前置兜底时高频简单意图的响应速度会受影响；
轻量化优化需额外工程成本：原生 BERT 模型体积大、推理慢，需专业算法工程师做模型轻量化、量化、工程加速优化，对团队技术能力有一定要求；
无法直接处理多意图输入：对车载场景中用户的多意图指令（如 “导航到上海并打开音乐”），需额外增加多意图识别模块，原生分类模型仅能输出单一意图。
# GPT 大模型
生成式大模型方案，通过 TF-IDF 计算待识别文本与训练集的相似样本，构造动态少样本提示词，调用 OpenAI 兼容接口实现意图分类，属于生成式语义分类方案。
- 核心优势
泛化能力极致，适配复杂车载场景：对车载语音的极端口语化、边缘表达、复杂长文本有超强的理解能力，能识别训练集中未标注的意图变体，大幅降低Other类占比，减少用户重复指令；
意图扩展成本为 0，适配业务快速迭代：新增 / 修改 20 + 核心意图，仅需修改提示词中的 “待选类别” 即可，无需补充标注数据、无需重新训练模型，是四大模型中意图扩展最便捷的方案；
原生支持多意图识别：可直接处理车载场景中用户的多意图指令（如 “我想导航到机场再把空调调到 26 度吹脚”），无需额外开发多意图模块，贴合实际用户使用习惯；
数据支持能力极致，挖掘深度需求：不仅能输出意图类别、概率，还能通过语义理解分析用户输入的潜在需求（如 “空调太冷了”→潜在意图 “空调调温（升高）”），为市场分析提供深度的用户需求洞察；
无需训练专属模型：依托预训练大模型的通用语义理解能力，仅需少量训练集做相似样本匹配，无需为车载意图单独训练大模型，节省训练算力与时间成本。
- 核心缺点
推理延迟远超项目要求：单次推理包含 “TF-IDF 相似计算 + 提示词构造 + API 网络请求 + 大模型推理”，即使本地部署小参数量大模型，耗时也在500ms~2s，API 调用则因网络波动可达数秒，完全不满足 < 400ms 的车载低延迟要求；
车载端本地部署难度极大：即使是 4B/7B 小参数量大模型，也需要 10GB 以上显存与高算力支持，目前主流车载芯片均无法满足，仅能通过云端 API 调用，存在网络依赖；
存在幻觉风险，准确率稳定性差：即使设置temperature=0，仍可能因提示词设计、相似样本干扰出现幻觉（如 “导航” 误判为 “媒体控制”），无法稳定保持 95%+ 的准确率，违背车载场景的高可靠性要求；
成本高，批量推理效率低：代码为单条文本独立构造提示词推理，无批量处理能力，面对车载海量用户请求时效率极低；云端 API 调用按 token 收费，长期使用成本高，本地部署则需高额的硬件算力投入；
依赖 TF-IDF 相似样本，存在性能瓶颈：模型的少样本推理效果完全依赖 TF-IDF 匹配的 Top10 相似样本，若 TF-IDF 无法匹配有效样本，提示词失效，大模型的分类准确率会大幅下降，相当于用传统方案的短板限制了大模型的优势；
存在安全风险，适配车载场景性差：易受提示词攻击 / 越狱影响，恶意输入会导致模型脱离意图识别任务输出无关内容；且大模型的推理过程为 “黑箱”，无法做精准的错误溯源，不符合汽车电子的安全规范；
工程集成复杂：需要对接大模型 API / 本地部署框架，还需处理网络波动、接口超时、生成结果解析等问题，工程集成与维护成本远高